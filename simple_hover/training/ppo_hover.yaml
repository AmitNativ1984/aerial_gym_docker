params:
  seed: 42
  algo:
    name: a2c_continuous                    # PPO for continuous action spaces

  model:
    name: continuous_a2c_logstd             # Outputs action mean + learned log std  

  load_checkpoint: False


  # Network architecture
  network:
    name: actor_critic                      # Policy and value share one network
    separate: False                         # Single network for both actor/critic
    space:
      continuous:
        mu_activation: None                 # No activation on action mean
        sigma_activation: None              # No activation of action std
        mu_init:
          name: default                     # Weight initialization for mean head
        sigma_init:
          name: const_initializer           # Weight initialization for std head
          val: 0                            # Initial std = exp(0) = 1.0
        fixed_sigma: True                   # Std independent of observation
    mlp:
      units: [128, 64, 32]                  # Neural network architecture
      d2rl: False                           # No dense residual connections
      activation: relu                      # Non-linearity between layers
      initializer:
        name: default                       # How weights are initialized
        scale: 2                            # Multiplier for init variance


  config:
    name: simple_hover                      # Experiment name for logging
    env_name: simple_hover_task             # Must match runner env registry name
    env_config:
      num_envs: 512                         # Number of envs simulated in parallel
      headless: True                        # Disable visual rendering
      use_warp: False                       # No Camera - Disable GPU-accelerated ray casting

    # Reward Scaling
    reward_shaper:
      scale_value: 1.0                      # Scales rewards to stabilize learning - depends on reward coefficients
    
    # PPO settings
    normalize_advantage: True               # Zero-mean unit-var advantages
    gamma: 0.99                             # Future reward weighting
    tau: 0.95                               # Bias-variance tradeoff for GAE
    ppo: True                               # Use clipped surrogate objective

    # Learning rate
    learning_rate: 3e-4                     # Step size for optimizer
    lr_schedule: adaptive                   # Adjusts LR during training
    kl_threshold: 0.016                     # KL target for LR adaptation

    # Training params
    save_best_after: 10                     # Epochs before tracking best model
    score_to_win: 100000                    # Early stopping threshold
    grad_norm: 1.0                          # Prevents exploding gradients
    entropy_coef: 0.001                     # Encourages exploration
    truncate_grads: True                    # Apply gradient clipping
    e_clip: 0.2                             # Limits policy update magnitude
    clip_value: False                       # Whether to clip value estimates

    # Batch settings
    num_actors: 512                         # Match num_envs for simplicity
    horizon_length: 256                     # Time steps per rollout. Longer horizon for stability tasks
    minibatch_size: 1024                    # Batch size for SGD updates
    mini_epochs: 4                          # SGD passes per policy update

    # Value function
    critic_coef: 2.0                        # Weighting of value loss term
    normalize_value: True                   # Normalizes value predictions
    value_bootstrap: True                   # Use value at episode truncation

    # Input normalization 
    normalize_input: True                   # Normalize observations

    # Other settings
    bounds_loss_coef: 0.0001                # No action bounds loss needed
    max_epochs: 500                         # Training iterations limit
    use_diagnostics: True                   # Extra logging for debugging

    # Player settings (for inference/testing)
    player:
      render: True                          # Show visualization during eval
      deterministic: True                   # Greedy action selection
      games_num: 100000                     # Number of test episodes