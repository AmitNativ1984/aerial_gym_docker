params:
  seed: 42
  algo:
    name: a2c_continuous                    # PPO for continuous action spaces

  model:
    name: continuous_a2c_logstd             # Outputs action mean + learned log std

  load_checkpoint: False


  # Standard MLP Network architecture (matching reference position_setpoint_task)
  network:
    name: mlp_actor_critic                  # Standard MLP network
    separate: False                         # Shared features between actor and critic

    space:
      continuous:
        mu_activation: None                 # No activation on action output (unbounded)
        sigma_activation: None
        mu_init:
          name: default
        sigma_init:
          name: const_initializer
          val: 0                            # Initial log_std = 0, so std = exp(0) = 1.0
        fixed_sigma: False                  # Learn std during training

    mlp:
      units: [256, 128, 64]                 # Hidden layer sizes (matching reference)
      d2rl: False                           # No D2RL architecture
      activation: elu                       # ELU activation (matching reference)
      initializer:
        name: default                       # Xavier/Glorot initialization
        scale: 2


  config:
    name: simple_hover_mlp                  # Experiment name for logging
    env_name: simple_hover_snn_task         # Task name (same task as SNN)
    env_config:
      num_envs: 4096                        # Number of parallel environments
      headless: True                        # Disable visual rendering
      use_warp: False                       # No camera rendering

    # Reward parameters - defined in task_config.py (potential-based reward)
    # R_total = -k_dist*dist² - k_angvel*||ω||²*proximity - k_jitter*||Δa|| + success_bonus
    # See simple_hover_snn/config/task_config.py for actual values

    # Reward Scaling (applied by rl_games after reward computation)
    reward_shaper:
      scale_value: 0.1                      # Scale rewards down (reference uses 0.1)

    # PPO settings
    normalize_advantage: True               # Zero-mean unit-var advantages
    gamma: 0.99                             # Future reward weighting
    tau: 0.95                               # Bias-variance tradeoff for GAE
    ppo: True                               # Use clipped surrogate objective

    # Learning rate
    learning_rate: 5e-4                     # Initial learning rate
    lr_schedule: linear                     # Linear decay (safer - no sudden LR spikes)

    # Entropy coefficient scheduling (decreases exploration over time)
    entropy_coef_schedule: linear           # Options: linear, adaptive, None
    entropy_coef_final: 0.00001             # Final entropy value (very low exploration)

    # Training params
    save_best_after: 10                     # Epochs before tracking best model
    save_frequency: 10                      # Save checkpoint every N epochs
    score_to_win: 100000                    # Early stopping threshold
    grad_norm: 1.0                          # Gradient clipping
    entropy_coef: 0.001                     # Starting entropy coefficient (moderate exploration, was 0.01)
    truncate_grads: True                    # Apply gradient clipping
    e_clip: 0.2                             # Limits policy update magnitude
    clip_value: False                       # Whether to clip value estimates

    # Batch settings (matching reference)
    num_actors: 4096                        # Match num_envs
    horizon_length: 128                     # Longer for sparse success bonus (60 steps to succeed)
    minibatch_size: 8192                    # Larger minibatches (reference: 8192, was 4096)
    mini_epochs: 4                          # SGD passes over data

    # Value function
    critic_coef: 2                          # Weighting of value loss term (reference: 2)
    normalize_value: True                   # Normalizes value predictions
    value_bootstrap: True                   # Use value at episode truncation

    # Input normalization
    normalize_input: True                   # Normalize observations (better for varied scales)

    # Other settings
    bounds_loss_coef: 0.0001                # Small bounds loss
    max_epochs:  600                        # Training iterations limit (reference: 400)
    use_diagnostics: True                   # Extra logging for debugging
    use_smooth_clamp: False                 # Don't use smooth clamping (reference: False)

    # Player settings (for inference/testing)
    player:
      render: True                          # Show visualization during eval
      deterministic: True                   # Greedy action selection
      games_num: 100000                     # Number of test episodes
