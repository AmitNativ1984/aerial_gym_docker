params:
  seed: 42
  algo:
    name: a2c_continuous                    # PPO for continuous action spaces

  model:
    name: continuous_a2c_logstd             # Outputs action mean + learned log std

  load_checkpoint: False


  # SNN Network architecture
  network:
    name: snn_actor_critic                  # Custom SNN network (registered in runner.py)

    # SNN-specific configuration
    snn:
      num_steps: 10                         # Number of SNN simulation timesteps per forward pass
      hidden_dims: [256, 128, 64]           # Hidden layer dimensions (matches MLP architecture)
      beta: 0.999                           # Membrane potential decay factor (0 = no memory, 1 = perfect memory)
      spike_grad: fast_sigmoid               # Surrogate gradient function for backprop (fast_sigmoid is more stable)
      reset_mechanism: subtract             # Reset mechanism after spike: "subtract" or "zero"
      reset_delay: False                    # Whether to delay reset by one timestep


  config:
    name: simple_hover_snn                  # Experiment name for logging
    env_name: simple_hover_snn_task         # Must match runner env registry name
    env_config:
      num_envs: 4096                        # Number of envs simulated in parallel
      headless: True                        # Disable visual rendering
      use_warp: False                       # No Camera - Disable GPU-accelerated ray casting

    # Reward parameters - defined in task_config.py (potential-based reward)
    # R_total = -k_dist*dist² - k_angvel*||ω||²*proximity - k_jitter*||Δa|| + success_bonus
    # See simple_hover_snn/config/task_config.py for actual values

    # Reward Scaling (matching MLP config for fair comparison)
    reward_shaper:
      scale_value: 0.1                      # Scale rewards down (matches MLP)

    # PPO settings
    normalize_advantage: True               # Zero-mean unit-var advantages
    gamma: 0.99                             # Future reward weighting
    tau: 0.95                               # Bias-variance tradeoff for GAE
    ppo: True                               # Use clipped surrogate objective

    # Learning rate (matching MLP config for fair comparison)
    learning_rate: 5e-4                     # Initial learning rate (matches MLP)
    lr_schedule: linear                     # Linear decay from initial LR to 0

    # Training params
    save_best_after: 10                     # Epochs before tracking best model
    save_frequency: 10                      # Save checkpoint every N epochs
    score_to_win: 100000                    # Early stopping threshold
    grad_norm: 1.0                          # Standard gradient clipping
    entropy_coef: 0.001                     # Starting entropy coefficient (matches MLP)

    # Entropy coefficient scheduling (matching MLP config)
    entropy_coef_schedule: linear           # Linear decay over training
    entropy_coef_final: 0.00001             # Final entropy value (very low exploration)

    truncate_grads: True                    # Apply gradient clipping
    e_clip: 0.2                             # Limits policy update magnitude
    clip_value: False                       # Whether to clip value estimates

    # Batch settings (matching MLP config for fair comparison)
    num_actors: 4096                        # Match num_envs
    horizon_length: 128                     # Steps collected per env before update (matches MLP)
    minibatch_size: 8192                    # Samples per SGD step (matches MLP)
    mini_epochs: 4                          # SGD passes over data

    # Value function
    critic_coef: 2.0                        # Weighting of value loss term
    normalize_value: True                   # Normalizes value predictions
    value_bootstrap: True                   # Use value at episode truncation

    # Input normalization
    normalize_input: True                   # Normalize observations

    # Other settings
    bounds_loss_coef: 0.0001                # Small bounds loss
    max_epochs: 600                         # Training iterations limit (matches MLP)
    use_diagnostics: True                   # Extra logging for debugging
    use_smooth_clamp: False                 # Don't use smooth clamping (matches MLP)

    # Player settings (for inference/testing)
    player:
      render: True                          # Show visualization during eval
      deterministic: True                   # Greedy action selection
      games_num: 100000                     # Number of test episodes
